{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as sp\n",
    "from scipy.fftpack import fft  \n",
    "from scipy.fftpack import fftfreq\n",
    "from scipy.fftpack import ifft\n",
    "import math \n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from functools import reduce\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1293437",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datadir = os.getcwd()\n",
    "label_dir = './휴먼이해2024'\n",
    "data_dir = './train_dataset'\n",
    "acc_dir = os.path.join(data_dir, 'mAcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffa961",
   "metadata": {},
   "source": [
    "# 1. Parquet 데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1830547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from datetime import datetime\n",
    "\n",
    "paths = [x for x in os.listdir(train_datadir) if 'user' in x and '-' in x]\n",
    "\n",
    "target_list = ['e4Hr', 'e4Temp', 'mAcc', 'mGps']\n",
    "# 유저 그룹 폴더\n",
    "for path in paths:\n",
    "    user_group =os.path.join(train_datadir, path)\n",
    "    \n",
    "    # 개별 유저 폴더\n",
    "    for user in os.listdir(user_group):\n",
    "        print('='*10 + user + '='*10)\n",
    "        ts_group = os.path.join(user_group, user)\n",
    "        \n",
    "        # 타임스탬프 폴더\n",
    "        for timestamp in tqdm(os.listdir(ts_group)):\n",
    "            timestamp_path = os.path.join(ts_group, timestamp)\n",
    "            ts_to_date = datetime.strftime(datetime.fromtimestamp(int(timestamp)), '%Y-%m-%d')\n",
    "\n",
    "            for target in target_list:\n",
    "                target_path = os.path.join(timestamp_path, target)\n",
    "                target_dirlist = os.listdir(target_path)\n",
    "\n",
    "                if len(target_dirlist) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 타겟 파일 폴더\n",
    "                timestamp_df = None\n",
    "                ts_list = []\n",
    "                date_list = []\n",
    "                user_list = []\n",
    "                for t_file in target_dirlist:\n",
    "                    t_sample = pd.read_csv(os.path.join(target_path, t_file))\n",
    "                    ts_list.extend([t_file.replace('.csv','')]*len(t_sample))\n",
    "                    date_list.extend([ts_to_date]*len(t_sample))\n",
    "                    user_list.extend([user]*len(t_sample))\n",
    "\n",
    "                    if timestamp_df is None:\n",
    "                        timestamp_df = t_sample\n",
    "                    else:\n",
    "                        timestamp_df = pd.concat([timestamp_df, t_sample])\n",
    "\n",
    "                timestamp_df['timestamp_head'] = ts_list\n",
    "                timestamp_df['user'] = user_list\n",
    "                timestamp_df['date'] = date_list\n",
    "                timestamp_df['timestamp_sum'] = timestamp_df['timestamp_head'].apply(int)+timestamp_df['timestamp']\n",
    "                timestamp_df = timestamp_df[['user', 'timestamp_head', 'timestamp_sum', 'date']+list(timestamp_df.columns)[:-4]]\n",
    "\n",
    "                # 타임스탬프당 하나씩 생성\n",
    "                table = pa.Table.from_pandas(timestamp_df)\n",
    "                save_dir = os.path.join(timestamp_path, timestamp+'_'+target+'.parquet')\n",
    "                pq.write_table(table, save_dir)\n",
    "\n",
    "timestamp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb620c",
   "metadata": {},
   "source": [
    "# 유저별 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece67ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [x for x in os.listdir(os.getcwd()) if 'user' in x and '-' in x]\n",
    "\n",
    "target_list = ['e4Hr', 'mAcc', 'mGps']\n",
    "timestamps = []\n",
    "for path in paths:\n",
    "    user_group =os.path.join(os.getcwd(), path)\n",
    "    \n",
    "    for user in [x for x in os.listdir(user_group) if 'parquet' not in x]:\n",
    "        print('='*10 + user + '='*10)\n",
    "        ts_group = os.path.join(user_group, user)\n",
    "        for target in target_list:\n",
    "            user_total = None\n",
    "            for timestamp in tqdm(os.listdir(ts_group)):\n",
    "                t_file = timestamp + '_' +target+'.parquet'\n",
    "                if not os.path.isfile(os.path.join(ts_group, timestamp, t_file)):\n",
    "                    continue\n",
    "\n",
    "                timestamp_df = pd.read_parquet(os.path.join(ts_group, timestamp, t_file))\n",
    "                if user_total is None:\n",
    "                    user_total = timestamp_df\n",
    "                else:\n",
    "                    user_total = pd.concat([user_total, timestamp_df])\n",
    "\n",
    "            # parquet으로 저장\n",
    "            table = pa.Table.from_pandas(user_total)\n",
    "            save_dir = os.path.join(data_dir, target, user+'_'+target+'_v1.parquet')\n",
    "            pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a91e1",
   "metadata": {},
   "source": [
    "# 유저 모아서 하나의 파일로 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['mGps', 'e4Hr'] # mAcc는 메모리 오류 발생\n",
    "timestamps = []\n",
    "version = 'v1'\n",
    "\n",
    "for target in targets:\n",
    "    print('='*10 + target + '='*10)\n",
    "    target_path = os.path.join(data_dir, target)\n",
    "    all_total = None\n",
    "    for user_data in tqdm([x for x in os.listdir(target_path) if version in x]):\n",
    "        user_df = pd.read_parquet(os.path.join(target_path, user_data))\n",
    "        if all_total is None:\n",
    "            all_total = user_df\n",
    "        else:\n",
    "            all_total = pd.concat([all_total, user_df], axis=0)\n",
    "\n",
    "    table = pa.Table.from_pandas(all_total)\n",
    "    save_dir = os.path.join(data_dir, target+'_'+version+'.parquet')\n",
    "    pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24286dcd",
   "metadata": {},
   "source": [
    "# Heatrate 전처리\n",
    "## 1) 분 단위 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = pd.read_parquet(os.path.join(data_dir, 'e4Hr_v1.parquet'))\n",
    "hr['datetime'] = hr['timestamp_sum'].apply(datetime.fromtimestamp).apply(str)\n",
    "hr['datetime'] = [':'.join(x.split(':')[:-1]) for x in hr['datetime']]\n",
    "hr = hr.groupby(by=['user','datetime']).apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "hr.columns.name=None\n",
    "\n",
    "# 데이터 저장하기\n",
    "table = pa.Table.from_pandas(hr)\n",
    "save_dir = os.path.join(data_dir, 'e4Hr_v2.parquet')\n",
    "pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38a3a8",
   "metadata": {},
   "source": [
    "# GPS 전처리\n",
    "## 1) 분 단위 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = pd.read_parquet(os.path.join(data_dir, 'mGps_v1.parquet'))\n",
    "gps['datetime'] = gps['timestamp_sum'].apply(datetime.fromtimestamp).apply(str)\n",
    "# h은 분단위로 샘플링\n",
    "gps['second'] = [(int(x.split(':')[-1])%5)*5 for x in tqdm(gps['datetime'])]\n",
    "gps['datetime'] = [':'.join(x.split(':')[:-1])+':'+('0'+str(y) if y<10 else str(y)) for x, y in tqdm(zip(gps['datetime'], gps['second']))]\n",
    "gps = gps.groupby(by=['user','datetime']).apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "gps.columns.name=None\n",
    "\n",
    "# 데이터 저장하기\n",
    "table = pa.Table.from_pandas(gps)\n",
    "save_dir = os.path.join(data_dir, 'mGps_v2.parquet')\n",
    "pq.write_table(table, save_dir)\n",
    "\n",
    "gps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe55fdf",
   "metadata": {},
   "source": [
    "## 2) 위도, 경도 데이터를 거리 데이터로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5973568",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'gps'\n",
    "data_dir =\"D:/ETRI_ERC_2_data/dataset/train/나현 전처리\"\n",
    "file_nm = 'mGps_v2.parquet'\n",
    "sav_dir = \"D:/ETRI_ERC_2_data/dataset/train/train_day\"\n",
    "\n",
    "gps = pd.read_parquet(os.path.join(data_dir, file_nm))\n",
    "gps['date'] =gps['datetime'].astype(str).apply(lambda x:x.split(\" \")[0])\n",
    "\n",
    "\n",
    "def measure(lat1, lon1, lat2, lon2):\n",
    "    R = 6378.137  # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat / 2) * math.sin(dLat / 2) + \\\n",
    "        math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * \\\n",
    "        math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = R * c\n",
    "    return d * 1000  # meters\n",
    "\n",
    "gps_distance = []\n",
    "for (k1, k2), df in gps.groupby(['user', 'date']):\n",
    "    lat = df['lat'].values\n",
    "    lon = df['lon'].values\n",
    "    gps_map = []\n",
    "    for i in range(len(lat)-1):\n",
    "        lat1, lon1 = lat[i], lon[i]\n",
    "        lat2, lon2 = lat[i+1], lon[i+1]\n",
    "        distance = measure(lat1, lon1, lat2, lon2)\n",
    "        gps_map.append(distance)\n",
    "    gps_map.insert(0, 0)\n",
    "    gps_distance.append(gps_map)\n",
    "\n",
    "# gps_distance\n",
    "gps_result = sum(gps_distance, [])\n",
    "gps['distance'] = gps_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eac785",
   "metadata": {},
   "source": [
    "## 3) 일(day) 단위 그룹핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_pt = pd.pivot_table(gps,\n",
    "                        values=['lat', 'lon', 'distance'],\n",
    "                        index=['user','date'],\n",
    "                        aggfunc={'lat':list,\n",
    "                                'lon':list,\n",
    "                                'distance':sum,})\n",
    "gps_pt['lat'].apply(np.array)\n",
    "gps_pt['lon'].apply(np.array)\n",
    "gps_pt.reset_index(inplace=True)\n",
    "\n",
    "display(gps_pt)\n",
    "# gps_pt['distance'] = [sum(x) for x in gps_pt['distance']]\n",
    "\n",
    "# parquet으로 저장\n",
    "print('Saving..')\n",
    "table = pa.Table.from_pandas(gps_pt)\n",
    "save_dir = os.path.join(sav_dir, target+'_day.parquet')\n",
    "pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 전처리\n",
    "paths = [x for x in os.listdir(data_dir) if 'user' in x and '-' in x]\n",
    "\n",
    "user_label = None\n",
    "print('Get labels from each user...')\n",
    "# 유저 그룹 폴더\n",
    "for path in paths:\n",
    "    user_group =os.path.join(path)\n",
    "    \n",
    "    # 개별 유저 폴더\n",
    "    for user in os.listdir(user_group):\n",
    "        print('='*10 + user + '='*10)\n",
    "        ts_group = os.path.join(user_group, user)\n",
    "        \n",
    "        # 타임스탬프 폴더\n",
    "        for timestamp in tqdm(os.listdir(ts_group)):\n",
    "            labels = pd.read_csv(os.path.join(ts_group, timestamp, timestamp+'_label.csv'))\n",
    "            user_list = [user]*len(labels)\n",
    "            datetime_list = [':'.join(str(datetime.fromtimestamp(x)).split(':')[:-1]) for x in labels['ts']]\n",
    "            \n",
    "            labels['user'] = user_list\n",
    "            labels['datetime'] = datetime_list\n",
    "            \n",
    "            labels = labels[['user','datetime', 'activity']]\n",
    "            \n",
    "            if user_label is None:\n",
    "                user_label = labels\n",
    "            else:\n",
    "                user_label = pd.concat([user_label, labels], axis=0)\n",
    "                \n",
    "    del user_group, ts_group, user_list, datetime_list, labels\n",
    "    gc.collect()\n",
    "    \n",
    "print('v1 Saving...')\n",
    "table = pa.Table.from_pandas(user_label)\n",
    "save_dir = os.path.join(data_dir, 'labels_v1.parquet')\n",
    "pq.write_table(table, save_dir)\n",
    "\n",
    "# 유저 라벨 리스트화\n",
    "print('Conver to list...')\n",
    "label_pt = pd.pivot_table(user_label,\n",
    "                        values=['activity'],\n",
    "                        index=['user','datetime'],\n",
    "                        aggfunc={'activity':list,})\n",
    "label_pt.reset_index(inplace=True)\n",
    "\n",
    "                \n",
    "# 유저 통합 라벨 생성\n",
    "print('v2 Saving...')\n",
    "table = pa.Table.from_pandas(label_pt)\n",
    "save_dir = os.path.join(data_dir, 'labels_v2.parquet')\n",
    "pq.write_table(table, save_dir)\n",
    "\n",
    "label_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711c31e",
   "metadata": {},
   "source": [
    "## 2) Activity가 2개 이상인 경우\n",
    " - 빈도 수가 적은 값으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_activity(x):\n",
    "    for k in x:\n",
    "        if k not in activity_count.keys():\n",
    "            return k\n",
    "    \n",
    "    if activity_count[x[0]] < activity_count[x[1]]:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "\n",
    "label_pt = pd.read_parquet(os.path.join(data_dir, 'labels_v2.parquet'))\n",
    "label_pt['activity'] = [select_activity(x) if len(x)==2 else x[0] for x in label_pt['activity']]\n",
    "\n",
    "# 유저 통합 라벨 생성\n",
    "print('v3 Saving...')\n",
    "table = pa.Table.from_pandas(label_pt)\n",
    "save_dir = os.path.join(data_dir, 'labels_v3.parquet')\n",
    "pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444b0d4",
   "metadata": {},
   "source": [
    "# Accelerator 전처리\n",
    "## 1) 초 단위 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [x for x in os.listdir(acc_dir) if 'v1' in x]\n",
    "\n",
    "for user in tqdm(user_list):\n",
    "    user_data = pd.read_parquet(os.path.join(acc_dir, user))\n",
    "    user_nm = str(user).split('_')[0]\n",
    "    \n",
    "    user_data['datetime'] = [datetime.fromtimestamp(x) for x in user_data['timestamp_sum']]\n",
    "    user_data['datetime'] = user_data['datetime'].apply(str)\n",
    "    user_data['datetime'] = [x.split('.')[0] for x in user_data['datetime']]\n",
    "    \n",
    "    user_data = user_data.groupby('datetime').apply(lambda x:x.iloc[0]).reset_index(drop=True)\n",
    "    \n",
    "    table = pa.Table.from_pandas(user_data)\n",
    "    save_dir = os.path.join(acc_dir, user_nm+'_mAcc_v2.parquet')\n",
    "    pq.write_table(table, save_dir)\n",
    "    \n",
    "    del user_data, user_nm, table, save_dir\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9eb3b",
   "metadata": {},
   "source": [
    "## 2) median 필터 및 버터워스 필터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(signal):\n",
    "    array=np.array(signal)   \n",
    "    med_filtered=sp.signal.medfilt(array, kernel_size=3)\n",
    "    return  med_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843657cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 30\n",
    "nyq=sampling_freq/float(2)\n",
    "freq1 = 0.3\n",
    "freq2 = 20\n",
    "\n",
    "# Function name: components_selection_one_signal\n",
    "\n",
    "# Inputs: t_signal:1D numpy array (time domain signal); \n",
    "\n",
    "# Outputs: (total_component,t_DC_component , t_body_component, t_noise) \n",
    "#           type(1D array,1D array, 1D array)\n",
    "\n",
    "# cases to discuss: if the t_signal is an acceleration signal then the t_DC_component is the gravity component [Grav_acc]\n",
    "#                   if the t_signal is a gyro signal then the t_DC_component is not useful\n",
    "# t_noise component is not useful\n",
    "# if the t_signal is an acceleration signal then the t_body_component is the body's acceleration component [Body_acc]\n",
    "# if the t_signal is a gyro signal then the t_body_component is the body's angular velocity component [Body_gyro]\n",
    "\n",
    "def components_selection_one_signal(t_signal,freq1,freq2):\n",
    "    t_signal=np.array(t_signal)\n",
    "    t_signal_length=len(t_signal)\n",
    "    f_signal=fft(t_signal)\n",
    "    freqs=np.array(sp.fftpack.fftfreq(t_signal_length, d=1/float(sampling_freq)))# frequency values between [-25hz:+25hz]\n",
    "    \n",
    "    # DC_component: f_signal values having freq between [-0.3 hz to 0 hz] and from [0 hz to 0.3hz] \n",
    "    #                                                             (-0.3 and 0.3 are included)\n",
    "    \n",
    "    # noise components: f_signal values having freq between [-25 hz to 20 hz[ and from ] 20 hz to 25 hz] \n",
    "    #                                                               (-25 and 25 hz inculded 20hz and -20hz not included)\n",
    "    \n",
    "    # selecting body_component: f_signal values having freq between [-20 hz to -0.3 hz] and from [0.3 hz to 20 hz] \n",
    "    #                                                               (-0.3 and 0.3 not included , -20hz and 20 hz included)\n",
    "    \n",
    "    \n",
    "    f_DC_signal=[] # DC_component in freq domain\n",
    "    f_body_signal=[] # body component in freq domain numpy.append(a, a[0])\n",
    "    f_noise_signal=[] # noise in freq domain\n",
    "    \n",
    "    for i in range(len(freqs)):# iterate over all available frequencies\n",
    "        \n",
    "        # selecting the frequency value\n",
    "        freq=freqs[i]\n",
    "        \n",
    "        # selecting the f_signal value associated to freq\n",
    "        value= f_signal[i]\n",
    "        \n",
    "        # Selecting DC_component values \n",
    "        if abs(freq)>0.3:# testing if freq is outside DC_component frequency ranges\n",
    "            f_DC_signal.append(float(0)) # add 0 to  the  list if it was the case (the value should not be added)                                       \n",
    "        else: # if freq is inside DC_component frequency ranges \n",
    "            f_DC_signal.append(value) # add f_signal value to f_DC_signal list\n",
    "    \n",
    "        # Selecting noise component values \n",
    "        if (abs(freq)<=20):# testing if freq is outside noise frequency ranges \n",
    "            f_noise_signal.append(float(0)) # # add 0 to  f_noise_signal list if it was the case \n",
    "        else:# if freq is inside noise frequency ranges \n",
    "            f_noise_signal.append(value) # add f_signal value to f_noise_signal\n",
    "\n",
    "        # Selecting body_component values \n",
    "        if (abs(freq)<=0.3 or abs(freq)>20):# testing if freq is outside Body_component frequency ranges\n",
    "            f_body_signal.append(float(0))# add 0 to  f_body_signal list\n",
    "        else:# if freq is inside Body_component frequency ranges\n",
    "            f_body_signal.append(value) # add f_signal value to f_body_signal list\n",
    "    \n",
    "    ################### Inverse the transformation of signals in freq domain ########################\n",
    "    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n",
    "    t_DC_component= ifft(np.array(f_DC_signal)).real\n",
    "    t_body_component= ifft(np.array(f_body_signal)).real\n",
    "    t_noise=ifft(np.array(f_noise_signal)).real\n",
    "    \n",
    "    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n",
    "                                     #  by substracting noise from t_signal (the original signal).\n",
    "    \n",
    "    # return outputs mentioned earlier\n",
    "    return (total_component,t_DC_component,t_body_component,t_noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ffaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [x for x in os.listdir(acc_dir) if 'v2' in x]\n",
    "\n",
    "for user in tqdm(user_list):\n",
    "    user_data = pd.read_parquet(os.path.join(acc_dir, user))\n",
    "    user_nm = str(user).split('_')[0]\n",
    "    \n",
    "    # median filter 적용\n",
    "    user_data['x'] = median(user_data['x'])\n",
    "    user_data['y'] = median(user_data['y'])\n",
    "    user_data['z'] = median(user_data['z'])\n",
    "    \n",
    "    user_data.rename(columns={'x':'x_med', 'y':'y_med', 'z':'z_med'}, inplace=True)\n",
    "    \n",
    "    # 성분 분해\n",
    "    (_,t_DC_component,t_body_component,_) = components_selection_one_signal(user_data['x_med'], freq1, freq2)\n",
    "    user_data['x_gravity'] = t_DC_component\n",
    "    user_data['x_body'] = t_body_component\n",
    "    (_,t_DC_component,t_body_component,_) = components_selection_one_signal(user_data['y_med'], freq1, freq2)\n",
    "    user_data['y_gravity'] = t_DC_component\n",
    "    user_data['y_body'] = t_body_component\n",
    "    (_,t_DC_component,t_body_component,_) = components_selection_one_signal(user_data['z_med'], freq1, freq2)\n",
    "    user_data['z_gravity'] = t_DC_component\n",
    "    user_data['z_body'] = t_body_component\n",
    "    \n",
    "    table = pa.Table.from_pandas(user_data)\n",
    "    save_dir = os.path.join(acc_dir, user_nm+'_mAcc_v3.parquet')\n",
    "    pq.write_table(table, save_dir)\n",
    "    \n",
    "    del user_data, user_nm, t_DC_component, t_body_component, table, save_dir\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0264a29",
   "metadata": {},
   "source": [
    "## 3) 일(day 단위로 합치기)\n",
    "## 예지 전처리 합친거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f08f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =\"D:/ETRI_ERC_2_data/dataset/train/나현 전처리\"\n",
    "user_list = [x for x in os.listdir(data_dir) if 'v3' in x]\n",
    "sav_dir = \"D:/ETRI_ERC_2_data/dataset/train/train_day\"\n",
    "\n",
    "target = 'acc'\n",
    "for user in user_list:\n",
    "    user_nm = user.split('_')[0]\n",
    "    print('='*10+user_nm+'='*10)\n",
    "    user_data = pd.read_parquet(os.path.join(data_dir, user))\n",
    "    user_data['date'] = user_data['datetime'].astype(str).apply(lambda x:x.split(\" \")[0])\n",
    "    # user_data['datetime'] = user_data['datetime'].astype(str)\n",
    "    user_pt = pd.pivot_table(user_data,\n",
    "                            values=['x_med','y_med', 'z_med', \n",
    "                                    'x_gravity', 'x_body', 'y_gravity', 'y_body', 'z_gravity', 'z_body', \n",
    "                                    'm_mag', 'g_mag', 'b_mag'],\n",
    "                            index=['user','date'],\n",
    "                            aggfunc={'x_med':list,\n",
    "                                    'y_med':list,\n",
    "                                    'z_med':list,\n",
    "                                    'x_gravity':list,\n",
    "                                    'x_body':list,\n",
    "                                    'y_gravity':list,\n",
    "                                    'y_body':list,\n",
    "                                    'z_gravity':list,\n",
    "                                    'z_body':list,\n",
    "                                    'm_mag':list,\n",
    "                                    'g_mag':list,\n",
    "                                    'b_mag':list,})\n",
    "    user_pt['x_med'].apply(np.array)\n",
    "    user_pt['y_med'].apply(np.array)\n",
    "    user_pt['z_med'].apply(np.array)\n",
    "    user_pt['x_gravity'].apply(np.array)\n",
    "    user_pt['y_gravity'].apply(np.array)\n",
    "    user_pt['z_gravity'].apply(np.array)\n",
    "    user_pt['x_body'].apply(np.array)\n",
    "    user_pt['y_body'].apply(np.array)\n",
    "    user_pt['z_body'].apply(np.array)\n",
    "    user_pt['m_mag'].apply(np.array)\n",
    "    user_pt['g_mag'].apply(np.array)\n",
    "    user_pt['b_mag'].apply(np.array)\n",
    "    user_pt.reset_index(inplace=True)\n",
    "\n",
    "    # parquet으로 저장\n",
    "    print('Saving..')\n",
    "    table = pa.Table.from_pandas(user_pt)\n",
    "    save_dir = os.path.join(sav_dir, user_nm+'_'+target+'_day.parquet')\n",
    "    pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49efcc",
   "metadata": {},
   "source": [
    "## 4) Accelerator 파일 하나로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc 통합 파일 생성\n",
    "data_dir = 'D:/ETRI_ERC_2_data/dataset/train/train_day'\n",
    "user_list = [x.split('_')[0] for x in os.listdir(os.path.join(data_dir)) if 'user' in x]\n",
    "\n",
    "acc = None\n",
    "for user in user_list:\n",
    "    user_acc = pd.read_parquet(os.path.join(data_dir, user+'_acc_day.parquet'))\n",
    "    if acc is None:\n",
    "        acc = user_acc\n",
    "    else:\n",
    "        acc = pd.concat([acc, user_acc], axis=0)\n",
    "        \n",
    "table = pa.Table.from_pandas(acc)\n",
    "save_dir = os.path.join(data_dir, 'acc_day.parquet')\n",
    "pq.write_table(table, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fcd8f",
   "metadata": {},
   "source": [
    "# train 유효한 유저-날짜 구하기\n",
    " - heatrate, accelerator, activity가 모두 있는 데이터를 사용\n",
    " - user25의 경우 이상치가 많아 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_files = [x for x in os.listdir(os.path.join(data_dir, 'mAcc')) if 'v3' in x]\n",
    "\n",
    "# Accelerator\n",
    "user_acc = None\n",
    "for user_f in user_files:\n",
    "    user_df = pd.read_parquet(os.path.join(data_dir, 'mAcc', user_f))\n",
    "    if user_acc is None:\n",
    "        user_acc = user_df\n",
    "    else:\n",
    "        user_acc = pd.concat([user_acc, user_df], axis=0)\n",
    "        \n",
    "acc_date = user_acc.groupby(['user', 'date'])['timestamp_sum'].count().reset_index()[['user', 'date']]\n",
    "acc_date.rename(columns={'date':'datetime'}, inplace=True)\n",
    "\n",
    "# Heartrate\n",
    "hr_df = pd.read_parquet(os.path.join(data_dir, 'e4Hr_v3.parquet'))\n",
    "hr_df['datetime'] = hr_df['datetime'].apply(lambda x: x.split()[0])\n",
    "hr_date = hr_df.groupby(['user', 'datetime']).count().reset_index()[['user', 'datetime']]\n",
    "\n",
    "# GPS\n",
    "gps_df = pd.read_parquet(os.path.join(data_dir, 'mGps_v2.parquet'))\n",
    "gps_date = gps_df.groupby(['user', 'date'])['timestamp'].count().reset_index()[['user', 'date']]\n",
    "gps_date.rename(columns={'date':'datetime'}, inplace=True)\n",
    "\n",
    "# Activity\n",
    "activity_df = pd.read_parquet(os.path.join(data_dir, 'activity_v3.parquet'))\n",
    "activity_df['datetime'] = activity_df['datetime'].apply(lambda x:x.split()[0])\n",
    "activity_date = activity_df.groupby(['user', 'datetime']).count().reset_index()[['user', 'datetime']]\n",
    "\n",
    "# train 레이블\n",
    "label_valid_date = pd.read_csv(os.path.join(label_dir, 'train_label.csv'), index_col=[0])\n",
    "label_valid_date = label_valid_date[['subject_id', 'date']]\n",
    "label_valid_date.rename(columns={'subject_id':'user', 'date':'datetime'}, inplace=True)\n",
    "\n",
    "# 유효한 날짜 구하기\n",
    "valid_date = reduce(lambda x,y: pd.merge(x, y, on=['user', 'datetime'], how='inner'),[acc_date, hr_date, gps_date, activity_date, label_valid_date])\n",
    "\n",
    "valid_date.to_csv(os.path.join(data_dir, 'train_valid_date.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f3cbe",
   "metadata": {},
   "source": [
    "## 데이터 최종 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_date = pd.read_csv(os.path.join(data_dir, 'train_valid_date.csv'))\n",
    "acc_df = pd.read_parquet(os.path.join(data_dir, 'acc_v3.parquet'))\n",
    "hr_df = pd.read_parquet(os.path.join(data_dir, 'e4Hr_v3.parquet'))\n",
    "act_df = pd.read_parquet(os.path.join(data_dir, 'activity_v3.parquet'))\n",
    "gps_df = pd.read_parquet(os.path.join(data_dir, 'mGps_v2.parquet'))\n",
    "\n",
    "acc_df = acc_df[['user','timestamp_sum', 'date', 'datetime', 'x_med',\n",
    "       'y_med', 'z_med', 'x_gravity', 'x_body', 'y_gravity',\n",
    "       'y_body', 'z_gravity', 'z_body', 'm_mag', 'g_mag', 'b_mag']]\n",
    "\n",
    "hr_df['date'] = hr_df['datetime'].apply(lambda x: x.split()[0])\n",
    "act_df['date'] = act_df['datetime'].apply(lambda x: x.split()[0])\n",
    "gps_df = gps_df[['user','timestamp_sum', 'date', 'datetime', 'lat', 'lon', 'accuracy']]\n",
    "\n",
    "# 유효한 날짜만 남기기\n",
    "valid_date['user-date'] = [str(x) + '_' + str(y) for x, y in zip(valid_date['user'], valid_date['datetime'])]\n",
    "acc_df['user-date'] = [str(x) + '_' + str(y) for x, y in zip(acc_df['user'], acc_df['date'])]\n",
    "hr_df['user-date'] = [str(x) + '_' + str(y) for x, y in zip(hr_df['user'], hr_df['date'])]\n",
    "act_df['user-date'] = [str(x) + '_' + str(y) for x, y in zip(act_df['user'], act_df['date'])]\n",
    "gps_df['user-date'] = [str(x) + '_' + str(y) for x, y in zip(gps_df['user'], gps_df['date'])]\n",
    "\n",
    "# user25 제거(acc 이상치 및 데이터 개수가 이상함)\n",
    "valid_date = valid_date[valid_date['user']!='user25']\n",
    "acc_df = acc_df[acc_df['user']!='user25']\n",
    "hr_df = hr_df[hr_df['user']!='user25']\n",
    "act_df = act_df[act_df['user']!='user25']\n",
    "gps_df = gps_df[gps_df['user']!='user25']\n",
    "\n",
    "# valid_date에 해당하는 데이터만 추출\n",
    "acc_valid = acc_df[acc_df['user-date'].isin(valid_date['user-date'])]\n",
    "hr_valid = hr_df[hr_df['user-date'].isin(valid_date['user-date'])]\n",
    "act_valid = act_df[act_df['user-date'].isin(valid_date['user-date'])]\n",
    "gps_valid = gps_df[gps_df['user-date'].isin(valid_date['user-date'])]\n",
    "\n",
    "acc_valid.drop(columns=['user-date'], inplace=True)\n",
    "hr_valid.drop(columns=['user-date'], inplace=True)\n",
    "act_valid.drop(columns=['user-date'], inplace=True)\n",
    "gps_valid.drop(columns=['user-date'], inplace=True)\n",
    "\n",
    "# valid 데이터 저장\n",
    "pq.write_table(pa.Table.from_pandas(acc_valid), os.path.join(data_dir, 'mAcc_valid.parquet'))\n",
    "pq.write_table(pa.Table.from_pandas(hr_valid), os.path.join(data_dir, 'e4Hr_valid.parquet'))\n",
    "pq.write_table(pa.Table.from_pandas(act_valid), os.path.join(data_dir, 'activity_valid.parquet'))\n",
    "pq.write_table(pa.Table.from_pandas(gps_valid), os.path.join(data_dir, 'mGps_valid.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
